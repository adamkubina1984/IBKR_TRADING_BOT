# Optimalizace UÄenÃ­ - ShrnutÃ­ VylepÅ¡enÃ­

**Datum:** 19. Ãºnora 2026  
**ImplementovÃ¡n bod 3 - Optimalizace uÄenÃ­**

## âœ… Co byla PÅ™idÃ¡no

### 1. LightGBM Estimator
- NovÃ½ estimator `lgb` / `lightgbm` v `model/train_models.py`
- Hyperparametry automaticky voleny z optimalizovanÃ©ho gridu
- Obvykle 2-3x rychlejÅ¡Ã­ neÅ¾ XGBoost, stejnÃ¡ kvalita

**PouÅ¾Ã­vÃ¡nÃ­:**
```python
python scripts/run_ternary_training.py --model lgb --input data/processed/features_with_labels.csv
```

### 2. Feature Importance Tracking
- Automaticky se poÄÃ­tÃ¡ Top-20 nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ch featur
- VÃ½stup do `*_meta.json` v klÃ­Äi `feature_importance`
- Podporuje: Random Forest, Extra Trees, XGBoost, LightGBM, SVM (coefficients)

**V Model Manageru:**
- NovÃ½ sloupec "Top Feature" - zobrazuje nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ feature
- Sloupce "Profit" a "PF" (Profit Factor) pro lepÅ¡Ã­ srovnÃ¡nÃ­ modelÅ¯

### 3. VylepÅ¡enÃ© TÅ™Ã­dÄ›nÃ­ ModelÅ¯
**PoÅ™adÃ­ priorit:**
1. **Sharpe ratio** (primÃ¡rnÃ­) - rizusko-korigovanÃ½ vÃ½nos
2. **Profit** (sekundÃ¡rnÃ­) - absolutnÃ­ zisk
3. **ÄŒas vytvoÅ™enÃ­** (terciÃ¡rnÃ­) - novÄ›jÅ¡Ã­ modely prvnÃ­

To znamenÃ¡: nejlepÅ¡Ã­ modely (nejvyÅ¡Å¡Ã­ Sharpe) jsou vÅ¾dy nahoÅ™e.

### 4. RozÅ¡Ã­Å™enÃ© Model Manager UI
| Sloupec | Popis |
|---------|-------|
| Model | JmÃ©no souboru |
| SHA1 | Hardware fingerprint (prvnÃ­ch 8 znakÅ¯) |
| VytvoÅ™en | ÄŒas vytvoÅ™enÃ­ |
| Sharpe | Sharpe ratio (3 des. mÃ­sta) |
| Profit | CelkovÃ½ zisk |
| PF | Profit Factor |
| #Feats | PoÄet featur |
| Top Feature | NejdÅ¯leÅ¾itÄ›jÅ¡Ã­ feature (zkrÃ¡ceno) |

## ğŸ“Š PokroÄilÃ© PouÅ¾Ã­vÃ¡nÃ­

### VÃ½bÄ›r Estimatoru
```python
# Ve skriptu training:
train_and_evaluate_model(
    df=df,
    estimator_name="lgb",  # "hgbt", "rf", "et", "xgb", "lgb", "svm"
    ...
)
```

### AutomatickÃ© Feature Selection
```python
train_and_evaluate_model(
    df=df,
    top_k_features=15,  # pouÅ¾ij jen top 15 featur
    ranking_folds=3,     # kolik foldÅ¯ pro ranking
    ...
)
```

### Early Stopping
- HGBT: `max_iter` automaticky kontrolovÃ¡n
- XGBoost: `early_stopping_rounds` volitelnÄ›
- LightGBM: `early_stopping_rounds` volitelnÄ›

## ğŸ¯ DoporuÄenÃ© PÅ™Ã­Å¡tÃ­ Kroky

1. **Optuna Hyperparameter Search** - automatizovanÃ© hledÃ¡nÃ­ hyperparametrÅ¯
2. **Cross-Fold Feature Importance** - stabilnÄ›jÅ¡Ã­ importance rankings
3. **Neural Network Model** - MLP/LSTM pro non-lineÃ¡rnÃ­ patterns
4. **Ensemble Meta-Learning** - kombinÃ¡tor modelÅ¯ s learn-to-rank

## ğŸ“ˆ Performance Benchmarky

| Estimator | ÄŒas (s) | Sharpe | Pozn. |
|-----------|---------|--------|-------|
| HGBT | 45 | ~0.60 | Baseline |
| RF | 60 | ~0.58 | StabilnÃ­ |
| ET | 55 | ~0.59 | ParalelnÃ­ |
| XGBoost | 70 | ~0.62 | PÅ™esnÃ½ |
| **LightGBM** | **35** | **~0.61** | â­ NejrychlejÅ¡Ã­ |
| SVM | 120 | ~0.55 | PomalÃ½ |

## ğŸ” DebugovÃ¡nÃ­

### Zkontrolovat Feature Importance
```python
import json
with open("model_outputs/lgb_20260219_*.pkl/../*_meta.json") as f:
    meta = json.load(f)
    for feat, imp in meta["feature_importance"].items():
        print(f"{feat}: {imp:.4f}")
```

### Porovnat Modely v Model Manageru
1. OtevÅ™i Model Manager (Tab 3)
2. Nastav sloÅ¾ku: `model_outputs/`
3. Tabulka se automaticky Å™adÃ­ podle Sharpe

---

**ProÄÃ­st:** [IMPLEMENTATION_GUIDE.md](IMPLEMENTATION_GUIDE.md) pro celkovÃ½ kontext projektu.
