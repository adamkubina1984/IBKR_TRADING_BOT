# model/data_split.py
# üü¶ Modul pro walk-forward validaci a export dataset≈Ø

import os

import pandas as pd


def walk_forward_split(df, window_size, test_size, step_size, expanding=False):
    """
    Rozdƒõl√≠ data na postupn√© tr√©novac√≠ a testovac√≠ sady (walk-forward).

    :param df: vstupn√≠ DataFrame s featurami a c√≠lovou promƒõnnou
    :param window_size: d√©lka tr√©novac√≠ho okna
    :param test_size: d√©lka validaƒçn√≠ho okna
    :param step_size: krok posunu okna
    :param expanding: True = expanding window, False = rolling window
    :return: seznam dvojic (train_df, test_df)
    """
    splits = []
    start = 0
    while start + window_size + test_size <= len(df):
        train_end = start + window_size
        test_end = train_end + test_size

        train_df = df.iloc[:train_end] if expanding else df.iloc[start:train_end]
        test_df = df.iloc[train_end:test_end]

        splits.append((train_df.copy(), test_df.copy()))
        start += step_size
    return splits

def export_datasets(pairs, output_dir, prefix="fold", format="csv"):
    """
    Ulo≈æ√≠ jednotliv√© dvojice (train, test) dataset≈Ø do soubor≈Ø.

    :param pairs: seznam dvojic (train_df, test_df)
    :param output_dir: c√≠lov√° slo≈æka pro export
    :param prefix: n√°zev souboru (prefix_foldX_train/test)
    :param format: csv nebo pkl
    """
    os.makedirs(output_dir, exist_ok=True)
    for i, (train_df, test_df) in enumerate(pairs):
        if format == "csv":
            train_df.to_csv(os.path.join(output_dir, f"{prefix}_{i}_train.csv"), index=False)
            test_df.to_csv(os.path.join(output_dir, f"{prefix}_{i}_test.csv"), index=False)
        elif format == "pkl":
            train_df.to_pickle(os.path.join(output_dir, f"{prefix}_{i}_train.pkl"))
            test_df.to_pickle(os.path.join(output_dir, f"{prefix}_{i}_test.pkl"))
        else:
            raise ValueError("Nepodporovan√Ω form√°t exportu")

# ibkr_trading_bot/model/train_models.py
#
# Popisky:
# - Robustn√≠ tr√©nov√°n√≠ s GridSearchCV pro XGB/LGBM/RF s ohledem na nevyv√°≈æen√° data.
# - Sanitizace featur (konstantn√≠/duplicitn√≠), ≈°ir≈°√≠ gridy, a optimalizace prahu rozhodnut√≠.
# - Z√°kladn√≠ (z√°chrann√©) ƒçasov√© featury, pokud dataset neobsahuje nic jin√©ho.
# - Ukl√°d√°me i 'decision_threshold' pro jednotn√© rozhodov√°n√≠ v dal≈°√≠ch ƒç√°stech projektu.

import joblib
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from xgboost import XGBClassifier

from ibkr_trading_bot.features.augmentations import add_noise, mix_dataframes, roll_shift
from ibkr_trading_bot.features.feature_engineering import prepare_dataset_with_targets
from ibkr_trading_bot.utils.io_helpers import load_dataframe


# --- Funkce pro generov√°n√≠ syntetick√Ωch dat ---
def generate_synthetic_data(df: pd.DataFrame, n_samples: int = 10, noise_level: float = 0.01) -> pd.DataFrame:
    """
    Vygeneruje n_samples syntetick√Ωch variant datasetu pomoc√≠:
    - p≈ôid√°n√≠ ≈°umu
    - n√°hodn√©ho posunu (rolling)
    - kombinace dvou dataset≈Ø (mixov√°n√≠)
    V√Ωstupem je dataframe p≈ôipraven√Ω pro tr√©nov√°n√≠ (prepare_dataset_with_targets).
    """
    synthetic = []
    for _ in range(n_samples):
        noisy = add_noise(df, noise_level=noise_level)
        shifted = roll_shift(noisy)
        synthetic.append(shifted)

    # Mixov√°n√≠ sousedn√≠ch vzork≈Ø (zv√Ω≈°√≠ diverzitu)
    for i in range(len(synthetic) - 1):
        mixed = mix_dataframes(synthetic[i], synthetic[i + 1], alpha=0.5)
        synthetic.append(mixed)

    result = pd.concat(synthetic).dropna()
    return prepare_dataset_with_targets(result)


def train_and_evaluate_model(X, y, model_name: str, param_grid: dict, window: str = None):
    """
    Tr√©nuje model s grid search a ukl√°d√° nejlep≈°√≠ model + v√Ωsledky.

    Args:
        X (pd.DataFrame): Vstupn√≠ featury
        y (pd.Series): C√≠lov√° promƒõnn√° (bin√°rn√≠ 0/1)
        model_name (str): 'xgb' | 'lgbm' | 'rf'
        param_grid (dict): Grid parametr≈Ø pro dan√Ω model
        window (str, optional): Oznaƒçen√≠ okna pro rolling retrain

    Returns:
        best_model: Natr√©novan√Ω model s nejlep≈°√≠mi parametry
        best_score: F1 sk√≥re z cross-validace nejlep≈°√≠ konfigurace
    """
    # --- Sanitizace a kontrola c√≠le + info o nevyv√°≈æenosti ---
    y_tmp = pd.Series(y).astype(int)
    if y_tmp.nunique() < 2:
        raise ValueError(
            "Stratifikovan√° CV vy≈æaduje alespo≈à 2 t≈ô√≠dy v 'y'. "
            "Zkontroluj p≈ô√≠pravu targetu nebo zvol jin√© obdob√≠/parametry."
        )
    min_class = y_tmp.value_counts().min() if not y_tmp.empty else 0
    n_splits = 3 if min_class >= 3 else 2  # fallback na 2, kdy≈æ je m√°lo vzork≈Ø
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    # scale_pos_weight pro XGBoost (vyv√°≈æen√≠ t≈ô√≠d)
    pos = int((y_tmp == 1).sum())
    neg = int((y_tmp == 0).sum())
    spw = (neg / max(pos, 1)) if (pos > 0 and neg > 0) else 1.0
    print(f"‚ÑπÔ∏è  T≈ô√≠dy v y: neg={neg}, pos={pos} | scale_pos_weight={spw:.3f}")

    # --- V√Ωbƒõr modelu s robustn√≠mi defaulty ---
    if model_name == "xgb":
        model = XGBClassifier(
            eval_metric="logloss",
            tree_method="hist",
            scale_pos_weight=spw,
            random_state=42,
            n_jobs=-1,
        )
    elif model_name == "lgbm":
        model = LGBMClassifier(
            objective="binary",
            num_leaves=31,
            min_data_in_leaf=5,       # men≈°√≠ listy pom≈Ø≈æou u mal√Ωch oken
            learning_rate=0.1,
            n_estimators=200,
            class_weight="balanced",  # kdy≈æ je target nevyv√°≈æen√Ω
            force_col_wise=True,      # stabilnƒõj≈°√≠/rychlej≈°√≠ na men≈°√≠ch tabulk√°ch
            verbosity=-1,
            n_jobs=-1,
            random_state=42,
        )
    elif model_name == "rf":
        model = RandomForestClassifier(
            random_state=42,
            class_weight="balanced",
            n_jobs=-1
        )
    else:
        raise ValueError(f"Nezn√°m√Ω model: {model_name}")

    # --- Fallback param_grid, pokud p≈ôijde pr√°zdn√Ω (nap≈ô. z GUI) ---
    if not param_grid:
        if model_name == "lgbm":
            param_grid = {
                "num_leaves": [15, 31, 63, 127],
                "min_data_in_leaf": [5, 10, 20, 40],
                "n_estimators": [200, 400, 700],
                "learning_rate": [0.03, 0.05, 0.1],
                "max_depth": [-1, 6, 10],
            }
        elif model_name == "xgb":
            param_grid = {
                "n_estimators": [200, 400, 700],
                "max_depth": [3, 5, 8],
                "learning_rate": [0.03, 0.05, 0.1],
                "subsample": [0.7, 0.9, 1.0],
                "colsample_bytree": [0.7, 0.9, 1.0],
            }
        elif model_name == "rf":
            param_grid = {
                "n_estimators": [300, 600, 1000],
                "max_depth": [None, 10, 16, 24],
                "min_samples_split": [2, 5, 10],
                "max_features": ["sqrt", "log2", None],
            }

    # --- Sanitizace X a odhoz nepou≈æiteln√Ωch featur ---
    X = pd.DataFrame(X).replace([float("inf"), float("-inf")], 0.0).fillna(0.0)

    # 1) drop konstantn√≠ch sloupc≈Ø
    const_cols = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]
    if const_cols:
        print(
            f"‚ö†Ô∏è  Odstra≈àuji {len(const_cols)} konstantn√≠ch featur: "
            f"{const_cols[:10]}{'...' if len(const_cols) > 10 else ''}"
        )
        X = X.drop(columns=const_cols)

    # 2) drop duplicitn√≠ch sloupc≈Ø (best-effort)
    try:
        unique_cols = X.T.drop_duplicates().T.columns
        if len(unique_cols) < X.shape[1]:
            removed = [c for c in X.columns if c not in unique_cols]
            print(
                f"‚ö†Ô∏è  Odstra≈àuji duplicitn√≠ featury: "
                f"{removed[:10]}{'...' if len(removed) > 10 else ''}"
            )
            X = X.loc[:, unique_cols]
    except Exception:
        pass

    if X.shape[1] == 0:
        raise ValueError("Po oƒçi≈°tƒõn√≠ nezbyly ≈æ√°dn√© featury. Zkontroluj feature engineering / konfiguraci.")

    # --- Grid Search ---
    grid = GridSearchCV(
        model,
        param_grid,
        cv=cv,
        scoring="f1",
        n_jobs=-1,
        error_score="raise",
    )
    grid.fit(X, y_tmp)


    best_model = grid.best_estimator_
    best_score = grid.best_score_

    # --- Optimalizace prahu rozhodnut√≠ na z√°kladƒõ F1 ---
    try:
        if hasattr(best_model, "predict_proba"):
            proba = best_model.predict_proba(X)[:, 1]
        elif hasattr(best_model, "decision_function"):
            import numpy as np
            raw = best_model.decision_function(X)
            raw = (raw - raw.min()) / (raw.max() - raw.min() + 1e-9)
            proba = raw
        else:
            proba = None

        best_threshold = 0.5
        best_f1 = -1.0
        if proba is not None:
            import numpy as np
            for t in np.linspace(0.05, 0.95, 37):
                tp = ((proba >= t) & (y_tmp == 1)).sum()
                fp = ((proba >= t) & (y_tmp == 0)).sum()
                fn = ((proba <  t) & (y_tmp == 1)).sum()
                precision = tp / max(tp + fp, 1)
                recall    = tp / max(tp + fn, 1)
                f1 = 2 * precision * recall / max(precision + recall, 1e-9)
                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = float(t)
        else:
            best_threshold = 0.5
            best_f1 = best_score

        print(f"üéØ Optimalizovan√Ω pr√°h rozhodnut√≠: {best_threshold:.3f} | F1@threshold‚âà{best_f1:.4f}")
    except Exception as e:
        print(f"‚ö†Ô∏è Optimalizace prahu selhala: {e}")
        best_threshold = 0.5

    # --- Ulo≈æen√≠ modelu ---
    suffix = f"_window{window}" if window else ""
    output_path = f"model_outputs/{model_name}{suffix}.pkl"
    os.makedirs("model_outputs", exist_ok=True)
    payload = {
        "model": best_model,
        "features": list(X.columns),
        "best_params": grid.best_params_,
        "decision_threshold": best_threshold,  # ulo≈æ√≠me prah
    }
    joblib.dump(payload, output_path)

    print(f"‚úÖ Model {model_name} ulo≈æen do {output_path} | F1 (CV): {best_score:.4f}")

    return best_model, best_score


def _select_feature_columns(df: pd.DataFrame) -> list:
    """
    Vybere numerick√© featury pro tr√©nink.
    1) Prim√°rnƒõ v≈°echny numerick√© sloupce mimo timestamp/target/y/signal a mimo syrov√© OHLC/volume.
    2) Pokud nic nezbyde, vytvo≈ô√≠ minim√°ln√≠ ‚Äûƒçasov√©‚Äú featury:
       - _ret1/_ret3/_ret5: 1/3/5-krokov√° n√°vratnost
       - _hl_range: high-low rozpƒõt√≠
       - _oc_change: close-open zmƒõna (absolutn√≠)
       - _vol_10: rolling volatilita (std z n√°vratnost√≠)
    """
    hard_blacklist = {"timestamp", "target", "y", "signal"}
    ohlc = {"open", "high", "low", "close", "volume"}

    candidates = [c for c in df.columns if c not in hard_blacklist and pd.api.types.is_numeric_dtype(df[c])]
    feat_cols = [c for c in candidates if c not in ohlc]

    if not feat_cols:
        if all(c in df.columns for c in ("close", "open", "high", "low")):
            df["_ret1"] = df["close"].pct_change().fillna(0.0)
            df["_ret3"] = df["close"].pct_change(3).fillna(0.0)
            df["_ret5"] = df["close"].pct_change(5).fillna(0.0)
            df["_hl_range"] = (df["high"] - df["low"]).fillna(0.0)
            df["_oc_change"] = (df["close"] - df["open"]).fillna(0.0)
            df["_vol_10"] = df["close"].pct_change().rolling(10).std().fillna(0.0)
            feat_cols = ["_ret1", "_ret3", "_ret5", "_hl_range", "_oc_change", "_vol_10"]
        else:
            if not candidates:
                raise ValueError("Nenalezeny ≈æ√°dn√© numerick√© sloupce pro tr√©nink.")
            feat_cols = candidates

    try:
        target_col = "target"
        if target_col in df.columns and feat_cols:
            corr = df[feat_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)
            leaky = corr[corr >= 0.999].index.tolist()
            if leaky:
                print(
                    f"‚ùóÔ∏è Odstra≈àuji potenci√°lnƒõ leakuj√≠c√≠ featury (|corr|‚â•0.999 s targetem): {leaky[:10]}{'...' if len(leaky) > 10 else ''}"
                )
                feat_cols = [c for c in feat_cols if c not in leaky]
                if not feat_cols:
                    raise ValueError("Po anti-leak filtru nezbyly ≈æ√°dn√© featury. Zkontroluj feature engineering.")
    except Exception:
        # Na selh√°n√≠ korelace nereagujeme tvrdƒõ; ponech√°me p≈Øvodn√≠ feat_cols
        pass

    return feat_cols


def train_simple_model(features_csv: str, model_out: str) -> str:
    """
    Jednoduch√Ω ‚Äûplug-and-play‚Äú tr√©nink volan√Ω z CLI:
      - naƒçte featury z CSV,
      - p≈ôiprav√≠ target (p≈ôes prepare_dataset_with_targets),
      - natr√©nuje rychl√Ω RF,
      - ulo≈æ√≠ model + seznam featur do joblib/pkl.
    Vrac√≠ cestu k ulo≈æen√©mu modelu.
    """
    if not os.path.exists(features_csv):
        raise FileNotFoundError(f"Soubor s featurami neexistuje: {features_csv}")

    raw = pd.read_csv(features_csv)
    dataset = prepare_dataset_with_targets(raw)

    if "target" not in dataset.columns:
        raise ValueError("Ve vstupn√≠m datasetu chyb√≠ sloupec 'target' po prepare_dataset_with_targets().")
    y = dataset["target"].astype(int)
    X_cols = _select_feature_columns(dataset)
    X = dataset[X_cols].replace([float("inf"), float("-inf")], 0.0).fillna(0.0)

    # Odhoƒè konstantn√≠ featury i tady (pro jistotu)
    const_cols = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]
    if const_cols:
        X = X.drop(columns=const_cols)

    model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)
    model.fit(X, y)

    out_dir = os.path.dirname(model_out)
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)
    joblib.dump({"model": model, "features": list(X.columns), "decision_threshold": 0.5}, model_out)

    print(f"‚úÖ Model ulo≈æen: {model_out} | n_features={X.shape[1]}")
    return model_out


# üü© Voliteln√© spu≈°tƒõn√≠ walk-forward splitu p≈ôes CLI
if __name__ == "__main__":
    import argparse

    import yaml

    parser = argparse.ArgumentParser()
    parser.add_argument("--split-data", action="store_true", help="Provede walk-forward split a export")
    parser.add_argument("--train-model", type=str, help="Spust√≠ tr√©nov√°n√≠ modelu (xgb, lgbm, rf)")
    args = parser.parse_args()

    if args.split_data:
        from ibkr_trading_bot.model.data_split import export_datasets, walk_forward_split

        here = os.path.dirname(os.path.abspath(__file__))             # .../ibkr_trading_bot/model
        cfg_path = os.path.normpath(os.path.join(here, "..", "config", "default_config.yaml"))
        with open(cfg_path, encoding="utf-8") as f:
            config = yaml.safe_load(f)
        split_cfg = config.get("data_split", {})

        window_size = split_cfg.get("window_size", 500)
        test_size = split_cfg.get("test_size", 100)
        step_size = split_cfg.get("step_size", 50)
        expanding = split_cfg.get("expanding", False)
        export_format = split_cfg.get("export_format", "csv")
        output_dir = split_cfg.get("output_dir", "data/control/")

        df = load_dataframe("data/processed/features.csv")
        splits = walk_forward_split(df, window_size, test_size, step_size, expanding)
        export_datasets(splits, output_dir=output_dir, format=export_format)

        print(f"‚úÖ Walk-forward split dokonƒçen. Exportov√°no {len(splits)} sad do: {output_dir}")

    if args.train_model:
        df = load_dataframe("data/processed/features.csv")
        dataset = prepare_dataset_with_targets(df)

        X_cols = _select_feature_columns(dataset)
        X = dataset[X_cols].replace([float("inf"), float("-inf")], 0.0).fillna(0.0)
        y = dataset["target"].astype(int)

        if args.train_model == "xgb":
            param_grid = {
                "n_estimators": [200, 400, 700],
                "max_depth": [3, 5, 8],
                "learning_rate": [0.03, 0.05, 0.1],
                "subsample": [0.7, 0.9, 1.0],
                "colsample_bytree": [0.7, 0.9, 1.0],
            }
        elif args.train_model == "lgbm":
            param_grid = {
                "n_estimators": [200, 400, 700],
                "num_leaves": [31, 63, 127],
                "learning_rate": [0.03, 0.05, 0.1],
                "max_depth": [-1, 6, 10],
                "min_data_in_leaf": [5, 10, 20, 40],
            }
        elif args.train_model == "rf":
            param_grid = {
                "n_estimators": [300, 600, 1000],
                "max_depth": [None, 10, 16, 24],
                "min_samples_split": [2, 5, 10],
                "max_features": ["sqrt", "log2", None],
            }
        else:
            raise ValueError(f"Model '{args.train_model}' nen√≠ podporov√°n.")

        train_and_evaluate_model(X, y, args.train_model, param_grid)


def split_by_calendar_days(
    df,
    train_days: int,
    test_days: int,
    timestamp_col: str = "timestamp",
    target_col: str = "target",
    feature_blacklist=None,
):
    """
    Deterministick√Ω split podle kalend√°≈ôn√≠ch dn≈Ø (od konce datasetu).
    - test = posledn√≠ch `test_days` dn√≠,
    - train = dn≈Ø bezprost≈ôednƒõ p≈ôed testem v poƒçtu `train_days`.
    Vrac√≠: X_train, y_train, X_test, y_test (sanitizovan√©, numerick√©).
    """
    import pandas as pd

    if feature_blacklist is None:
        feature_blacklist = ["timestamp", "signal", "y"]
    if target_col not in feature_blacklist:
        feature_blacklist = list(set(feature_blacklist + [target_col]))

    if timestamp_col not in df.columns:
        raise ValueError(f"Ve vstupn√≠m DF chyb√≠ sloupec '{timestamp_col}'.")
    if target_col not in df.columns:
        raise ValueError(f"Ve vstupn√≠m DF chyb√≠ sloupec '{target_col}'.")

    dfx = df.copy()
    dfx[timestamp_col] = pd.to_datetime(dfx[timestamp_col])
    dfx = dfx.sort_values(timestamp_col).reset_index(drop=True)

    # seznam unik√°tn√≠ch kalend√°≈ôn√≠ch dn≈Ø v po≈ôad√≠
    dfx["_date"] = dfx[timestamp_col].dt.date
    unique_days = dfx["_date"].drop_duplicates().tolist()

    need = train_days + test_days
    if len(unique_days) < need:
        raise ValueError(f"Nedostatek dn≈Ø v datasetu: k dispozici {len(unique_days)}, pot≈ôeba {need}.")

    test_days_list = unique_days[-test_days:]
    train_days_list = unique_days[-(test_days + train_days):-test_days]

    df_train = dfx[dfx["_date"].isin(train_days_list)].copy()
    df_test  = dfx[dfx["_date"].isin(test_days_list)].copy()

    print("üóìÔ∏è V√Ωbƒõr podle dn≈Ø:")
    print(f"  ‚Ä¢ Train [{len(train_days_list)}]: {train_days_list[0]} ‚Üí {train_days_list[-1]}")
    print(f"  ‚Ä¢ Test  [{len(test_days_list)}]: {test_days_list[0]} ‚Üí {test_days_list[-1]}")
    print(f"  ‚Ä¢ Poƒçty ≈ô√°dk≈Ø: train={len(df_train)}, test={len(df_test)}")

    # v√Ωbƒõr numerick√Ωch featur mimo blacklist (a preferenƒçnƒõ mimo syrov√© OHLC)
    hard_blacklist = set(feature_blacklist + ["_date"])
    ohlc = {"open", "high", "low", "close", "volume"}

    candidates = [c for c in df_train.columns
                  if c not in hard_blacklist and pd.api.types.is_numeric_dtype(df_train[c])]
    feat_cols = [c for c in candidates if c not in ohlc] or candidates
    if not feat_cols:
        raise ValueError("Nenalezeny pou≈æiteln√© featury po aplikaci blacklistu.")

    def sanitize(m: pd.DataFrame) -> pd.DataFrame:
        m = m[feat_cols].replace([float("inf"), float("-inf")], 0.0).fillna(0.0)
        const_cols = [c for c in m.columns if m[c].nunique(dropna=False) <= 1]
        if const_cols:
            m = m.drop(columns=const_cols)
        return m

    X_train = sanitize(df_train)
    X_test  = sanitize(df_test)
    y_train = df_train[target_col].astype(int)
    y_test  = df_test[target_col].astype(int)

    if X_train.shape[1] == 0 or X_test.shape[1] == 0:
        raise ValueError("Po oƒçi≈°tƒõn√≠ nezbyly ≈æ√°dn√© featury pro train/test.")

    return X_train, y_train, X_test, y_test


